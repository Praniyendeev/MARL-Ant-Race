{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2071ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (1.22.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from pettingzoo) (1.24.2)\n",
      "Requirement already satisfied: gymnasium>=0.26.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from pettingzoo) (0.28.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (2.2.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (4.4.0)\n",
      "Requirement already satisfied: pygame in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: torch in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo\n",
    "!pip install pygame\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1094b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries and create the simple tag environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Create the simple tag environment\n",
    "env = simple_tag_v2.parallel_env(num_obstacles = 0, max_cycles = 50, render_mode=\"human\")\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5632b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8816341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, lr_actor, lr_critic, agent, device):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(46, 20, hidden_dim).to(device)\n",
    "        self.target_critic = Critic(46, 20, hidden_dim).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.name = agent\n",
    "        \n",
    "        self.device = device\n",
    "        self.update_target_networks()\n",
    "        \n",
    "\n",
    "    def update_target_networks(self):\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def act(self, state, noise=None):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy()\n",
    "        if noise:\n",
    "            action += noise.noise()\n",
    "        return action\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + source_param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3f2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_maddpg(agents, env, episodes, noise, device, batch_size=128, gamma=0.99, tau=0.05):\n",
    "    memory = deque(maxlen=100000)\n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : agent.act(states[agent.name], noise) for agent in agents}\n",
    "            action_vals = {agent.name : np.argmax(actions[agent.name]) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(action_vals)\n",
    "            env.render()\n",
    "            \n",
    "            memory.append((states, actions, rewards, next_states, dones))\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "        \n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "        \n",
    "            if len(memory) >= batch_size: \n",
    "                experiences = random.sample(memory, batch_size)\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "                    \n",
    "                # Extracting and organizing data from memory.\n",
    "                actors_states = {}\n",
    "                actors_actions = {}\n",
    "                actors_rewards = {}\n",
    "                actors_next_states = {}\n",
    "                actors_dones = {}\n",
    "                                \n",
    "                for agent in env.agents:\n",
    "                    actors_states[agent] = torch.stack([torch.from_numpy(batch_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_actions[agent] = torch.stack([torch.from_numpy(batch_actions[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_rewards[agent] = torch.Tensor([batch_rewards[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_next_states[agent] = torch.stack([torch.from_numpy(batch_next_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_dones[agent] = torch.Tensor([batch_dones[itr][agent]*1 for itr in range(batch_size)])\n",
    "        \n",
    "                batch_states = torch.cat([actors_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_next_states = torch.cat([actors_next_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_actions = torch.cat([actors_actions[agent] for agent in env.agents], dim = 1)\n",
    "        \n",
    "                # Preparing data for Critic Model\n",
    "#                 batch_states = torch.from_numpy(batch_states).float().to(device)\n",
    "#                 batch_actions = torch.from_numpy(batch_actions).float().to(device)\n",
    "#                 batch_next_states = torch.from_numpy(batch_next_states).float().to(device)\n",
    "                \n",
    "#                 batch_dones = torch.from_numpy(np.array(batch_dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "                for agent in agents:\n",
    "                    # Update the critic\n",
    "                    next_actions = [a.target_actor(actors_next_states[a.name]) for a in agents]\n",
    "                    next_actions = torch.cat(next_actions, dim=1)\n",
    "                    target_q_values = agent.target_critic(batch_next_states, next_actions)\n",
    "                    expected_q_values = actors_rewards[agent.name][:,None] + (1 - actors_dones[agent.name][:,None]) * gamma * target_q_values\n",
    "                    q_values = agent.critic(batch_states, batch_actions)\n",
    "                    critic_loss = F.mse_loss(q_values, expected_q_values.detach())\n",
    "                    agent.critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    agent.critic_optimizer.step()\n",
    "\n",
    "                    # Update the actor\n",
    "                    actions = [a.actor(actors_states[a.name]) for a in agents]\n",
    "                    actions = torch.cat(actions, dim=1)\n",
    "                    actor_loss = -agent.critic(batch_states, actions).mean()\n",
    "                    agent.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    agent.actor_optimizer.step()\n",
    "\n",
    "                    # Update target networks\n",
    "                    soft_update(agent.target_actor, agent.actor, tau)\n",
    "                    soft_update(agent.target_critic, agent.critic, tau)\n",
    "                    \n",
    "            states = next_states\n",
    "\n",
    "        rewards_list.append(episode_rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n",
    "\n",
    "    return rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c1feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maddpg(agents, env, episodes):\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : np.argmax(agent.act(states[agent.name])) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(actions)\n",
    "            env.render()\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "\n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "\n",
    "            states = next_states\n",
    "            time.sleep(0.05)\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49a1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "episodes = 2000\n",
    "test_episodes = 10\n",
    "\n",
    "# Create the agents\n",
    "agents = [MADDPGAgent(state_dims[agent], action_dim, hidden_dim, lr_actor, lr_critic, agent, device) for agent in env.agents]\n",
    "\n",
    "# Create noise for exploration\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "# Train the agents\n",
    "rewards = train_maddpg(agents, env, episodes, noise, device)\n",
    "\n",
    "for agent in agents:\n",
    "    torch.save(agent.actor.state_dict(), f\"simple_tag_models/{agent.name}_actor.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"simple_tag_models/{agent.name}_critic.pth\")\n",
    "\n",
    "for agent in agents:\n",
    "        agent.actor.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_actor.pth\"))\n",
    "    \n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)\n",
    "\n",
    "# Save Rewards\n",
    "np.save(f\"simple_tag_models/rewards.npy\", np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba2acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 12, 'adversary_1': 12, 'adversary_2': 12, 'agent_0': 10} 5 \n",
      " <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "adversary_0\n",
      "adversary_1\n",
      "adversary_2\n",
      "agent_0\n",
      "Test Episode 1/100, Reward: 0.0\n",
      "Test Episode 2/100, Reward: 39.56343979500046\n",
      "Test Episode 3/100, Reward: -0.5008571162884323\n",
      "Test Episode 4/100, Reward: 0.0\n",
      "Test Episode 5/100, Reward: -5.140840906992892\n",
      "Test Episode 6/100, Reward: -12.31518798825121\n",
      "Test Episode 7/100, Reward: 0.0\n",
      "Test Episode 8/100, Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal_Models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_critic.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Test the trained agents\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtest_maddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mtest_maddpg\u001b[0;34m(agents, env, episodes)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     states \u001b[38;5;241m=\u001b[39m next_states\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_rewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e3\n",
    "episodes = 5000\n",
    "test_episodes = 100\n",
    "\n",
    "# Create the agents\n",
    "agents = [MADDPGAgent(state_dims[agent], action_dim, hidden_dim, lr_actor, lr_critic, agent, device) for agent in env.agents]\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent.name)\n",
    "    agent.actor.load_state_dict(torch.load(f\"Final_Models/{agent.name}_actor.pth\", map_location=torch.device('cpu')))\n",
    "    agent.critic.load_state_dict(torch.load(f\"Final_Models/{agent.name}_critic.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
