{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b906cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries and create the simple tag environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Create the simple tag environment\n",
    "env = simple_tag_v2.parallel_env(num_obstacles = 0, max_cycles=50, render_mode=\"human\")\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6780e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, agent, eps_clip, K_epochs, device):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(46).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.name = agent\n",
    "        self.device = device        \n",
    "\n",
    "#     def get_action(self, state):\n",
    "#         state = torch.from_numpy(state).float().to(device)\n",
    "#         action_probs = self.actor(state)\n",
    "#         print(action_probs)\n",
    "#         action_dist = Categorical(action_probs)\n",
    "#         print(action_dist)\n",
    "#         action = action_dist.sample()\n",
    "#         return action.item()\n",
    "\n",
    "    def act(self, state, noise=None):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy()\n",
    "        if noise:\n",
    "            action += noise.noise()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b27423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mappo(agents, env, episodes, noise, device, batch_size=128, gamma=0.99, tau=0.05):\n",
    "    memory = deque(maxlen=100000)\n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : agent.act(states[agent.name], noise) for agent in agents}\n",
    "            action_vals = {agent.name : np.argmax(actions[agent.name]) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(action_vals)\n",
    "            env.render()\n",
    "#             print(actions)\n",
    "            \n",
    "            memory.append((states, action_vals, rewards, next_states, dones))\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "        \n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "        \n",
    "            if len(memory) >= batch_size: \n",
    "                experiences = random.sample(memory, batch_size)\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "                    \n",
    "                # Extracting and organizing data from memory.\n",
    "                actors_states = {}\n",
    "                actors_actions = {}\n",
    "                actors_rewards = {}\n",
    "                actors_next_states = {}\n",
    "                actors_dones = {}\n",
    "                                \n",
    "                for agent in env.agents:\n",
    "                    actors_states[agent] = torch.stack([torch.from_numpy(batch_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_actions[agent] = torch.Tensor([batch_actions[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_rewards[agent] = torch.Tensor([batch_rewards[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_next_states[agent] = torch.stack([torch.from_numpy(batch_next_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_dones[agent] = torch.Tensor([batch_dones[itr][agent]*1 for itr in range(batch_size)])\n",
    "        \n",
    "                batch_states = torch.cat([actors_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_next_states = torch.cat([actors_next_states[agent] for agent in env.agents], dim = 1)\n",
    "                \n",
    "                for agent in agents:\n",
    "                    for _ in range(agent.K_epochs):\n",
    "                        # Calculate advantages\n",
    "                        state_values = agent.critic(batch_states)\n",
    "                        next_state_values = agent.critic(batch_next_states).detach()\n",
    "                        advantages = actors_rewards[agent.name][:,None] + (1 - actors_dones[agent.name])[:,None] * gamma * next_state_values[:,None] - state_values[:,None]\n",
    "\n",
    "                        # Update the critic\n",
    "                        critic_loss = advantages.pow(2).mean()\n",
    "                        agent.critic_optimizer.zero_grad()\n",
    "                        critic_loss.backward()\n",
    "                        agent.critic_optimizer.step()\n",
    "                        \n",
    "                        # Calculate the new action probabilities and the old action probabilities\n",
    "                        new_action_probs = agent.actor(actors_states[agent.name])\n",
    "                        old_action_probs = new_action_probs.detach()\n",
    "                        new_action_probs = torch.sum(new_action_probs*torch.nn.functional.one_hot(actors_actions[agent.name].long(), num_classes = 5), 1)\n",
    "                        old_action_probs = torch.sum(old_action_probs*torch.nn.functional.one_hot(actors_actions[agent.name].long(), num_classes = 5), 1)\n",
    "                                                \n",
    "                        # Calculate the surrogate loss for the actor\n",
    "                        ratio = (new_action_probs / old_action_probs).exp()\n",
    "                        surrogate1 = ratio * advantages.detach()\n",
    "                        surrogate2 = torch.clamp(ratio, 1 - agent.eps_clip, 1 + agent.eps_clip) * advantages.detach()\n",
    "                        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "                        # Update the actor\n",
    "                        agent.actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        agent.actor_optimizer.step()\n",
    "                    \n",
    "            states = next_states\n",
    "\n",
    "        rewards_list.append(episode_rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n",
    "\n",
    "    return rewards_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7b5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.5, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c6627a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 12, 'adversary_1': 12, 'adversary_2': 12, 'agent_0': 10} 5 \n",
      " <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "Episode 1/2000, Reward: 38.388647583897175\n",
      "Episode 2/2000, Reward: -39.24935065129754\n",
      "Episode 3/2000, Reward: -20.8372535180986\n",
      "Episode 4/2000, Reward: -435.3506330150692\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m noise \u001b[38;5;241m=\u001b[39m OUNoise(action_dim)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the agents\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mappo\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m     26\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(agent\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_tag_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_actor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m, in \u001b[0;36mtrain_mappo\u001b[0;34m(agents, env, episodes, noise, device, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;66;03m# Update the actor\u001b[39;00m\n\u001b[1;32m     69\u001b[0m             agent\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m             \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m             agent\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m states \u001b[38;5;241m=\u001b[39m next_states\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "episodes = 2000\n",
    "test_episodes = 10\n",
    "eps_clip = 0.4\n",
    "K = 64\n",
    "\n",
    "# Create the agents\n",
    "agents = [MAPPOAgent(state_dims[agent], action_dim, lr_actor, lr_critic, agent, eps_clip, K, device) for agent in env.agents]\n",
    "\n",
    "# Create noise for exploration\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "# Train the agents\n",
    "rewards = train_mappo(agents, env, episodes, noise, device)\n",
    "\n",
    "for agent in agents:\n",
    "    torch.save(agent.actor.state_dict(), f\"simple_tag_models/{agent.name}_actor.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"simple_tag_models/{agent.name}_critic.pth\")\n",
    "\n",
    "for agent in agents:\n",
    "        agent.actor.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_actor.pth\"))\n",
    "    \n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)\n",
    "\n",
    "# Save Rewards\n",
    "np.save(f\"simple_tag_models/rewards.npy\", np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3084bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
