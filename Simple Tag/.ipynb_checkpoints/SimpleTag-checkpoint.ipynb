{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed1660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (1.22.3)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from pettingzoo) (1.24.2)\n",
      "Requirement already satisfied: gymnasium>=0.26.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from pettingzoo) (0.28.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (2.2.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from gymnasium>=0.26.0->pettingzoo) (4.4.0)\n",
      "Requirement already satisfied: pygame in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: torch in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/rajatyagi/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo\n",
    "!pip install pygame\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a3b65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries and create the simple tag environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Create the simple tag environment\n",
    "env = simple_tag_v2.parallel_env(num_obstacles = 3, max_cycles=50, render_mode=\"human\")\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe03b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "499eb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, lr_actor, lr_critic, agent, device):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(70, 20, hidden_dim).to(device)\n",
    "        self.target_critic = Critic(70, 20, hidden_dim).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.name = agent\n",
    "        \n",
    "        self.device = device\n",
    "        self.update_target_networks()\n",
    "        \n",
    "\n",
    "    def update_target_networks(self):\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def act(self, state, noise=None):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy()\n",
    "        if noise:\n",
    "            action += noise.noise()\n",
    "        return action\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + source_param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32759947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_maddpg(agents, env, episodes, noise, device, batch_size=128, gamma=0.99, tau=0.05):\n",
    "    memory = deque(maxlen=100000)\n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : agent.act(states[agent.name], noise) for agent in agents}\n",
    "            action_vals = {agent.name : np.argmax(actions[agent.name]) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(action_vals)\n",
    "#             env.render()\n",
    "            \n",
    "            memory.append((states, actions, rewards, next_states, dones))\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "        \n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "        \n",
    "            if len(memory) >= batch_size: \n",
    "                experiences = random.sample(memory, batch_size)\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "                    \n",
    "                # Extracting and organizing data from memory.\n",
    "                actors_states = {}\n",
    "                actors_actions = {}\n",
    "                actors_rewards = {}\n",
    "                actors_next_states = {}\n",
    "                actors_dones = {}\n",
    "                                \n",
    "                for agent in env.agents:\n",
    "                    actors_states[agent] = torch.stack([torch.from_numpy(batch_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_actions[agent] = torch.stack([torch.from_numpy(batch_actions[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_rewards[agent] = torch.Tensor([batch_rewards[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_next_states[agent] = torch.stack([torch.from_numpy(batch_next_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_dones[agent] = torch.Tensor([batch_dones[itr][agent]*1 for itr in range(batch_size)])\n",
    "        \n",
    "                batch_states = torch.cat([actors_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_next_states = torch.cat([actors_next_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_actions = torch.cat([actors_actions[agent] for agent in env.agents], dim = 1)\n",
    "        \n",
    "                # Preparing data for Critic Model\n",
    "#                 batch_states = torch.from_numpy(batch_states).float().to(device)\n",
    "#                 batch_actions = torch.from_numpy(batch_actions).float().to(device)\n",
    "#                 batch_next_states = torch.from_numpy(batch_next_states).float().to(device)\n",
    "                \n",
    "#                 batch_dones = torch.from_numpy(np.array(batch_dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "                for agent in agents:\n",
    "                    # Update the critic\n",
    "                    next_actions = [a.target_actor(actors_next_states[a.name]) for a in agents]\n",
    "                    next_actions = torch.cat(next_actions, dim=1)\n",
    "                    target_q_values = agent.target_critic(batch_next_states, next_actions)\n",
    "                    expected_q_values = actors_rewards[agent.name][:,None] + (1 - actors_dones[agent.name][:,None]) * gamma * target_q_values\n",
    "                    q_values = agent.critic(batch_states, batch_actions)\n",
    "                    critic_loss = F.mse_loss(q_values, expected_q_values.detach())\n",
    "                    agent.critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    agent.critic_optimizer.step()\n",
    "\n",
    "                    # Update the actor\n",
    "                    actions = [a.actor(actors_states[a.name]) for a in agents]\n",
    "                    actions = torch.cat(actions, dim=1)\n",
    "                    actor_loss = -agent.critic(batch_states, actions).mean()\n",
    "                    agent.actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    agent.actor_optimizer.step()\n",
    "\n",
    "                    # Update target networks\n",
    "                    soft_update(agent.target_actor, agent.actor, tau)\n",
    "                    soft_update(agent.target_critic, agent.critic, tau)\n",
    "                    \n",
    "            states = next_states\n",
    "\n",
    "        rewards_list.append(episode_rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n",
    "\n",
    "    return rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3348047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_maddpg(agents, env, episodes):\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : np.argmax(agent.act(states[agent.name])) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(actions)\n",
    "            env.render()\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "\n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "\n",
    "            states = next_states\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10aead9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e02a587",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "episodes = 2000\n",
    "test_episodes = 10\n",
    "\n",
    "# Create the agents\n",
    "agents = [MADDPGAgent(state_dims[agent], action_dim, hidden_dim, lr_actor, lr_critic, agent, device) for agent in env.agents]\n",
    "\n",
    "# Create noise for exploration\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "# Train the agents\n",
    "rewards = train_maddpg(agents, env, episodes, noise, device)\n",
    "\n",
    "for agent in agents:\n",
    "    torch.save(agent.actor.state_dict(), f\"simple_tag_models/{agent.name}_actor.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"simple_tag_models/{agent.name}_critic.pth\")\n",
    "\n",
    "for agent in agents:\n",
    "        agent.actor.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_actor.pth\"))\n",
    "    \n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)\n",
    "\n",
    "# Save Rewards\n",
    "np.save(f\"simple_tag_models/rewards.npy\", np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d95d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 12, 'adversary_1': 12, 'adversary_2': 12, 'agent_0': 10} 5 \n",
      " <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "adversary_0\n",
      "adversary_1\n",
      "adversary_2\n",
      "agent_0\n",
      "Test Episode 1/100, Reward: -4873.094272649464\n",
      "Test Episode 2/100, Reward: -4924.55821315223\n",
      "Test Episode 3/100, Reward: -5171.792812069902\n",
      "Test Episode 4/100, Reward: -4854.449733963409\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     agent\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_tag_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_critic.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Test the trained agents\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtest_maddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtest_maddpg\u001b[0;34m(agents, env, episodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {agent\u001b[38;5;241m.\u001b[39mname : np\u001b[38;5;241m.\u001b[39margmax(agent\u001b[38;5;241m.\u001b[39mact(states[agent\u001b[38;5;241m.\u001b[39mname])) \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents}\n\u001b[0;32m----> 8\u001b[0m     next_states, rewards, _, dones, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     10\u001b[0m     episode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mlist\u001b[39m(rewards\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/utils/conversions.py:157\u001b[0m, in \u001b[0;36maec_to_parallel_wrapper.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magent_selection\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m obs, rew, termination, truncation, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mlast()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    159\u001b[0m     rewards[agent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maec_env\u001b[38;5;241m.\u001b[39mrewards[agent]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/order_enforcing.py:75\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:108\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_selection\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/assert_out_of_bounds.py:26\u001b[0m, in \u001b[0;36mAssertOutOfBoundsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m         action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         action\n\u001b[1;32m     25\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction is not in action space\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/utils/wrappers/base.py:108\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39magent_selection\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrewards\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/mpe/_mpe_utils/simple_env.py:263\u001b[0m, in \u001b[0;36mSimpleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_rewards()\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/pettingzoo/mpe/_mpe_utils/simple_env.py:279\u001b[0m, in \u001b[0;36mSimpleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_render(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode)\n\u001b[0;32m--> 279\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e3\n",
    "episodes = 5000\n",
    "test_episodes = 100\n",
    "\n",
    "# Create the agents\n",
    "agents = [MADDPGAgent(state_dims[agent], action_dim, hidden_dim, lr_actor, lr_critic, agent, device) for agent in env.agents]\n",
    "\n",
    "for agent in agents:\n",
    "    print(agent.name)\n",
    "    agent.actor.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_actor.pth\"))\n",
    "    agent.critic.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_critic.pth\"))\n",
    "\n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
