{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b906cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries and create the simple tag environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from collections import deque\n",
    "import random\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Create the simple tag environment\n",
    "env = simple_tag_v2.parallel_env(num_obstacles = 0, max_cycles=50, render_mode=\"human\")\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6780e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, agent, eps_clip, K_epochs, device):\n",
    "        self.actor = Actor(state_dim, action_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        \n",
    "        self.critic = Critic(46).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        self.name = agent\n",
    "        self.device = device        \n",
    "\n",
    "#     def get_action(self, state):\n",
    "#         state = torch.from_numpy(state).float().to(device)\n",
    "#         action_probs = self.actor(state)\n",
    "#         print(action_probs)\n",
    "#         action_dist = Categorical(action_probs)\n",
    "#         print(action_dist)\n",
    "#         action = action_dist.sample()\n",
    "#         return action.item()\n",
    "\n",
    "    def act(self, state, noise=None):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy()\n",
    "        if noise:\n",
    "            action += noise.noise()\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7b27423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mappo(agents, env, episodes, noise, device, batch_size=128, gamma=0.99, tau=0.05):\n",
    "    memory = deque(maxlen=100000)\n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        episode_rewards = 0\n",
    "\n",
    "        while True:\n",
    "            actions = {agent.name : agent.act(states[agent.name], noise) for agent in agents}\n",
    "            action_vals = {agent.name : np.argmax(actions[agent.name]) for agent in agents}\n",
    "            next_states, rewards, _, dones, _ = env.step(action_vals)\n",
    "            env.render()\n",
    "#             print(actions)\n",
    "            \n",
    "            memory.append((states, action_vals, rewards, next_states, dones))\n",
    "            episode_rewards += np.sum(list(rewards.values()))\n",
    "        \n",
    "            if all(value == True for value in dones.values()):\n",
    "                break\n",
    "        \n",
    "            if len(memory) >= batch_size: \n",
    "                experiences = random.sample(memory, batch_size)\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "                    \n",
    "                # Extracting and organizing data from memory.\n",
    "                actors_states = {}\n",
    "                actors_actions = {}\n",
    "                actors_rewards = {}\n",
    "                actors_next_states = {}\n",
    "                actors_dones = {}\n",
    "                                \n",
    "                for agent in env.agents:\n",
    "                    actors_states[agent] = torch.stack([torch.from_numpy(batch_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_actions[agent] = torch.Tensor([batch_actions[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_rewards[agent] = torch.Tensor([batch_rewards[itr][agent] for itr in range(batch_size)])\n",
    "                    actors_next_states[agent] = torch.stack([torch.from_numpy(batch_next_states[itr][agent]).float().to(device) for itr in range(batch_size)])\n",
    "                    actors_dones[agent] = torch.Tensor([batch_dones[itr][agent]*1 for itr in range(batch_size)])\n",
    "        \n",
    "                batch_states = torch.cat([actors_states[agent] for agent in env.agents], dim = 1)\n",
    "                batch_next_states = torch.cat([actors_next_states[agent] for agent in env.agents], dim = 1)\n",
    "                \n",
    "                for agent in agents:\n",
    "                    for _ in range(agent.K_epochs):\n",
    "                        # Calculate advantages\n",
    "                        state_values = agent.critic(batch_states)\n",
    "                        next_state_values = agent.critic(batch_next_states).detach()\n",
    "                        advantages = actors_rewards[agent.name][:,None] + (1 - actors_dones[agent.name])[:,None] * gamma * next_state_values[:,None] - state_values[:,None]\n",
    "\n",
    "                        # Update the critic\n",
    "                        critic_loss = advantages.pow(2).mean()\n",
    "                        agent.critic_optimizer.zero_grad()\n",
    "                        critic_loss.backward()\n",
    "                        agent.critic_optimizer.step()\n",
    "                        \n",
    "                        # Calculate the new action probabilities and the old action probabilities\n",
    "                        new_action_probs = agent.actor(actors_states[agent.name])\n",
    "                        old_action_probs = new_action_probs.detach()\n",
    "                        new_action_probs = torch.sum(new_action_probs*torch.nn.functional.one_hot(actors_actions[agent.name].long(), num_classes = 5), 1)\n",
    "                        old_action_probs = torch.sum(old_action_probs*torch.nn.functional.one_hot(actors_actions[agent.name].long(), num_classes = 5), 1)\n",
    "                                                \n",
    "                        # Calculate the surrogate loss for the actor\n",
    "                        ratio = (new_action_probs / old_action_probs).exp()\n",
    "                        surrogate1 = ratio * advantages.detach()\n",
    "                        surrogate2 = torch.clamp(ratio, 1 - agent.eps_clip, 1 + agent.eps_clip) * advantages.detach()\n",
    "                        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "                        # Update the actor\n",
    "                        agent.actor_optimizer.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        agent.actor_optimizer.step()\n",
    "                    \n",
    "            states = next_states\n",
    "\n",
    "        rewards_list.append(episode_rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Reward: {episode_rewards}\")\n",
    "\n",
    "    return rewards_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d7b5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.5, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63c6627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': 12, 'adversary_1': 12, 'adversary_2': 12, 'agent_0': 10} 5 \n",
      " <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "Episode 1/2000, Reward: -104.75804775955447\n",
      "Episode 2/2000, Reward: -23.69604049074809\n",
      "Episode 3/2000, Reward: -27.11355955524026\n",
      "Episode 4/2000, Reward: -66.26501479531304\n",
      "Episode 5/2000, Reward: -317.7102066939231\n",
      "Episode 6/2000, Reward: -22.25582448105389\n",
      "Episode 7/2000, Reward: -11.30350216134312\n",
      "Episode 8/2000, Reward: -29.931055776407714\n",
      "Episode 9/2000, Reward: -41.58050321652352\n",
      "Episode 10/2000, Reward: 34.684186626174004\n",
      "Episode 11/2000, Reward: 0.0\n",
      "Episode 12/2000, Reward: 0.0\n",
      "Episode 13/2000, Reward: -12.47362604240265\n",
      "Episode 14/2000, Reward: -58.60399308628546\n",
      "Episode 15/2000, Reward: -24.87889047831528\n",
      "Episode 16/2000, Reward: 0.0\n",
      "Episode 17/2000, Reward: 0.0\n",
      "Episode 18/2000, Reward: 0.0\n",
      "Episode 19/2000, Reward: 0.0\n",
      "Episode 20/2000, Reward: 59.98144997411807\n",
      "Episode 21/2000, Reward: -0.6009422994151326\n",
      "Episode 22/2000, Reward: 0.0\n",
      "Episode 23/2000, Reward: -8.60013362983446\n",
      "Episode 24/2000, Reward: -18.37072246401319\n",
      "Episode 25/2000, Reward: 0.0\n",
      "Episode 26/2000, Reward: 0.0\n",
      "Episode 27/2000, Reward: -1.535987540304552\n",
      "Episode 28/2000, Reward: 0.0\n",
      "Episode 29/2000, Reward: 0.0\n",
      "Episode 30/2000, Reward: -27.139092338746767\n",
      "Episode 31/2000, Reward: -13.94690814945966\n",
      "Episode 32/2000, Reward: -14.759335260581341\n",
      "Episode 33/2000, Reward: -16.707084103882185\n",
      "Episode 34/2000, Reward: 0.0\n",
      "Episode 35/2000, Reward: 0.0\n",
      "Episode 36/2000, Reward: 0.0\n",
      "Episode 37/2000, Reward: -18.52580313848381\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m noise \u001b[38;5;241m=\u001b[39m OUNoise(action_dim)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the agents\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mappo\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(agent\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple_tag_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_actor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 70\u001b[0m, in \u001b[0;36mtrain_mappo\u001b[0;34m(agents, env, episodes, noise, device, batch_size, gamma, tau)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;66;03m# Update the actor\u001b[39;00m\n\u001b[1;32m     69\u001b[0m             agent\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m             \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m             agent\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     73\u001b[0m states \u001b[38;5;241m=\u001b[39m next_states\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_agents = len(env.agents)\n",
    "state_dims = {agent : env.observation_space(agent).shape[0] for agent in env.agents}\n",
    "action_dim = env.action_space('agent_0').n\n",
    "\n",
    "print(state_dims, action_dim, '\\n',type(env.action_space('adversary_0')))\n",
    "\n",
    "hidden_dim = 128\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "episodes = 2000\n",
    "test_episodes = 10\n",
    "eps_clip = 0.4\n",
    "\n",
    "# Create the agents\n",
    "agents = [MAPPOAgent(state_dims[agent], action_dim, lr_actor, lr_critic, agent, eps_clip, 1, device) for agent in env.agents]\n",
    "\n",
    "# Create noise for exploration\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "# Train the agents\n",
    "rewards = train_mappo(agents, env, episodes, noise, device)\n",
    "\n",
    "for agent in agents:\n",
    "    torch.save(agent.actor.state_dict(), f\"simple_tag_models/{agent.name}_actor.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"simple_tag_models/{agent.name}_critic.pth\")\n",
    "\n",
    "for agent in agents:\n",
    "        agent.actor.load_state_dict(torch.load(f\"simple_tag_models/{agent.name}_actor.pth\"))\n",
    "    \n",
    "# Test the trained agents\n",
    "test_maddpg(agents, env, test_episodes)\n",
    "\n",
    "# Save Rewards\n",
    "np.save(f\"simple_tag_models/rewards.npy\", np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3084bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
